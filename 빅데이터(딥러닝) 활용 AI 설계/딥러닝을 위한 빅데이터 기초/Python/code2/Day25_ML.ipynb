{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개의 범주형 자료 → 가변수 생성\n",
    "df = pd.DataFrame({\"m_id\": [1, 2, 3, 4, 5],\n",
    "                   \"m_gen\": [\"rock\", \"rock\", \"pop\", \"disco\", \"pop\"]},\n",
    "                  columns=[\"m_id\", \"m_gen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_id</th>\n",
       "      <th>m_gen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>disco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   m_id  m_gen\n",
       "0     1   rock\n",
       "1     2   rock\n",
       "2     3    pop\n",
       "3     4  disco\n",
       "4     5    pop"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5\n",
       "0  1  0  0  0  0\n",
       "1  0  1  0  0  0\n",
       "2  0  0  1  0  0\n",
       "3  0  0  0  1  0\n",
       "4  0  0  0  0  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = pd.get_dummies(df[\"m_id\"])\n",
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disco</th>\n",
       "      <th>pop</th>\n",
       "      <th>rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   disco  pop  rock\n",
       "0      0    0     1\n",
       "1      0    0     1\n",
       "2      0    1     0\n",
       "3      1    0     0\n",
       "4      0    1     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = pd.get_dummies(df['m_gen'])\n",
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_id</th>\n",
       "      <th>m_gen</th>\n",
       "      <th>genre_disco</th>\n",
       "      <th>genre_pop</th>\n",
       "      <th>genre_rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>rock</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rock</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pop</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>disco</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>pop</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   m_id  m_gen  genre_disco  genre_pop  genre_rock\n",
       "0     1   rock            0          0           1\n",
       "1     2   rock            0          0           1\n",
       "2     3    pop            0          1           0\n",
       "3     4  disco            1          0           0\n",
       "4     5    pop            0          1           0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm = df.join(mm.add_prefix(\"genre_\")) # 접두어를 생성하여 join\n",
    "mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./res/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./res/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./res/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./res/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./res/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5만개 train 이미지(입력, 레이블) → 모델 <br>\n",
    "1만개 test 이미지 → 모델 → 출력 결과 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10 # 분류결과의 종류 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "w = tf.Variable(tf.random_normal([28*28, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "hf= tf.nn.softmax(tf.matmul(x, w)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "is_correct = tf.equal(tf.arg_max(hf, 1), tf.arg_max(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "# 에폭 : 전체 데이터를 1번 트레이닝 → 1 에폭\n",
    "training_epochs = 15 # 15번 학습\n",
    "batch_size = 100 # 한번에 학습할 데이터 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 cost: 2.782539983\n",
      "Epoch    2 cost: 1.145379660\n",
      "Epoch    3 cost: 0.907995003\n",
      "Epoch    4 cost: 0.796263846\n",
      "Epoch    5 cost: 0.725168954\n",
      "Epoch    6 cost: 0.673101274\n",
      "Epoch    7 cost: 0.636264582\n",
      "Epoch    8 cost: 0.604817065\n",
      "Epoch    9 cost: 0.578997736\n",
      "Epoch   10 cost: 0.558487084\n",
      "Epoch   11 cost: 0.539195485\n",
      "Epoch   12 cost: 0.523211108\n",
      "Epoch   13 cost: 0.508411059\n",
      "Epoch   14 cost: 0.496989780\n",
      "Epoch   15 cost: 0.483461856\n",
      "Learning finished\n",
      "accuracy 0.8917\n",
      "Label: [0]\n",
      "prediction: [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs): # 5만 개 이미지 15번 트레이닝\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)  # 5만 / 100\n",
    "        for i in range(total_batch): # 500 번\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) # 다음 배치를 알아서 읽음\n",
    "            cv, _ = sess.run([cost, train], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += cv/total_batch # 100개 데이터에 대한 코스트/500 → cost의 평균\n",
    "            # 1, 3, 7, 10, 14 : 평균 35/5 = 7\n",
    "            # 0.2+0.6+1.4+2+2.8 = 7\n",
    "        # 에폭 때마다 cost 출력\n",
    "        print(\"Epoch %4d\" % (epoch+1), \n",
    "             \"cost: {:.9f}\".format(avg_cost))\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    print(\"accuracy\", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "    \n",
    "    r = random.randint(0, mnist.test.num_examples-1) # 테스트를 하고자하는 이미지 랜덤 선택\n",
    "    print(\"Label:\", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"prediction:\", sess.run(tf.argmax(hf, 1), feed_dict={x: mnist.test.images[r:r+1]}))\n",
    "    plt.style.use(\"default\")\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata =[[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "ydata = [[0], [1], [1], [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "w = tf.Variable(tf.random_normal([2, 1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "hf = tf.sigmoid(tf.matmul(x, w)+b)\n",
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "predict = tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2158568 [[-0.2247671 ]\n",
      " [-0.34134668]]\n",
      "100 0.7017315 [[0.3682344 ]\n",
      " [0.30010813]]\n",
      "200 0.6969496 [[0.25604627]\n",
      " [0.2196242 ]]\n",
      "300 0.69488347 [[0.17071709]\n",
      " [0.15125553]]\n",
      "400 0.69393873 [[0.1139401 ]\n",
      " [0.10354248]]\n",
      "500 0.69350785 [[0.07619791]\n",
      " [0.07064308]]\n",
      "600 0.6933115 [[0.05104736]\n",
      " [0.04807984]]\n",
      "700 0.69322205 [[0.03424858]\n",
      " [0.03266327]]\n",
      "800 0.6931813 [[0.02300569]\n",
      " [0.02215879]]\n",
      "900 0.6931627 [[0.01546864]\n",
      " [0.01501621]]\n",
      "1000 0.6931542 [[0.010409  ]\n",
      " [0.01016731]]\n",
      "1100 0.6931504 [[0.0070087 ]\n",
      " [0.00687961]]\n",
      "1200 0.6931487 [[0.00472153]\n",
      " [0.00465256]]\n",
      "1300 0.69314784 [[0.00318203]\n",
      " [0.00314517]]\n",
      "1400 0.6931475 [[0.00214516]\n",
      " [0.00212547]]\n",
      "1500 0.69314736 [[0.00144652]\n",
      " [0.00143601]]\n",
      "1600 0.6931473 [[0.00097562]\n",
      " [0.00096999]]\n",
      "1700 0.69314724 [[0.00065806]\n",
      " [0.00065507]]\n",
      "1800 0.6931472 [[0.00044403]\n",
      " [0.00044243]]\n",
      "1900 0.6931472 [[0.00029958]\n",
      " [0.00029873]]\n",
      "2000 0.6931472 [[0.00020214]\n",
      " [0.00020168]]\n",
      "2100 0.6931472 [[0.0001364 ]\n",
      " [0.00013614]]\n",
      "2200 0.6931472 [[9.203589e-05]\n",
      " [9.190292e-05]]\n",
      "2300 0.6931472 [[6.210690e-05]\n",
      " [6.203502e-05]]\n",
      "2400 0.6931472 [[4.1914336e-05]\n",
      " [4.1872259e-05]]\n",
      "2500 0.6931472 [[2.8278286e-05]\n",
      " [2.8255577e-05]]\n",
      "2600 0.6931472 [[1.9082781e-05]\n",
      " [1.9070503e-05]]\n",
      "2700 0.6931472 [[1.28675038e-05]\n",
      " [1.28611855e-05]]\n",
      "2800 0.69314724 [[8.689218e-06]\n",
      " [8.687370e-06]]\n",
      "2900 0.6931472 [[5.8535293e-06]\n",
      " [5.8531718e-06]]\n",
      "3000 0.6931472 [[3.9521378e-06]\n",
      " [3.9517804e-06]]\n",
      "3100 0.6931472 [[2.6527587e-06]\n",
      " [2.6524012e-06]]\n",
      "3200 0.69314724 [[1.8093500e-06]\n",
      " [1.8089926e-06]]\n",
      "3300 0.6931472 [[1.2133013e-06]\n",
      " [1.2129439e-06]]\n",
      "3400 0.6931472 [[8.1394785e-07]\n",
      " [8.1359047e-07]]\n",
      "3500 0.6931471 [[5.218845e-07]\n",
      " [5.215271e-07]]\n",
      "3600 0.6931472 [[3.6542227e-07]\n",
      " [3.6506486e-07]]\n",
      "3700 0.6931472 [[2.2982118e-07]\n",
      " [2.2946378e-07]]\n",
      "3800 0.6931472 [[1.6127558e-07]\n",
      " [1.6091818e-07]]\n",
      "3900 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4000 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4100 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4200 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4300 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4400 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4500 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4600 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4700 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4800 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "4900 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5000 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5100 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5200 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5300 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5400 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5500 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5600 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5700 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5800 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "5900 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6000 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6100 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6200 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6300 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6400 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6500 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6600 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6700 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6800 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "6900 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7000 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7100 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7200 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7300 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7400 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7500 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7600 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7700 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7800 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "7900 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8000 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8100 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8200 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8300 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8400 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8500 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8600 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8700 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8800 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "8900 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9000 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9100 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9200 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9300 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9400 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9500 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9600 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9700 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9800 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "9900 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "10000 0.6931472 [[1.3296327e-07]\n",
      " [1.3260586e-07]]\n",
      "hf [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "pre [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "acc 0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: xdata, y: ydata})\n",
    "        if step%100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x : xdata, y: ydata}),\n",
    "                 sess.run(w))\n",
    "    hv, pv, av = sess.run([hf, predict, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"hf\\n\", hv, \"\\npre\", pv, \"\\nacc\", av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN 으로 재구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 2]))\n",
    "b1 = tf.Variable(tf.random_normal([2]))\n",
    "L1 = tf.sigmoid(tf.matmul(x, w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([2, 1]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "hf = tf.sigmoid(tf.matmul(L1, w2)+b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "predict = tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 cost 0.7150096 [[-0.06941011  0.17842817]\n",
      " [-0.9415158  -1.1356446 ]] [[ 0.07614742]\n",
      " [-0.30379725]]\n",
      "Step 100 cost 0.6940909 [[-0.06935269  0.15178372]\n",
      " [-0.94225574 -1.116329  ]] [[-0.01682661]\n",
      " [-0.37786907]]\n",
      "Step 200 cost 0.69380677 [[-0.07141014  0.11326114]\n",
      " [-0.9422161  -1.1065173 ]] [[-0.02099586]\n",
      " [-0.35858867]]\n",
      "Step 300 cost 0.69361514 [[-0.07361941  0.07577328]\n",
      " [-0.9422165  -1.0995075 ]] [[-0.02118803]\n",
      " [-0.33911702]]\n",
      "Step 400 cost 0.6934533 [[-0.07584715  0.03956899]\n",
      " [-0.9422362  -1.0945402 ]] [[-0.02149208]\n",
      " [-0.32374227]]\n",
      "Step 500 cost 0.69331175 [[-0.07811547  0.00424891]\n",
      " [-0.9422722  -1.0912992 ]] [[-0.02202494]\n",
      " [-0.31244418]]\n",
      "Step 600 cost 0.6931823 [[-0.08044566 -0.0306065 ]\n",
      " [-0.94232327 -1.0895727 ]] [[-0.02271644]\n",
      " [-0.30507395]]\n",
      "Step 700 cost 0.6930581 [[-0.08285116 -0.06541964]\n",
      " [-0.9423885  -1.0892279 ]] [[-0.02349184]\n",
      " [-0.30154833]]\n",
      "Step 800 cost 0.69293237 [[-0.0853373  -0.10062493]\n",
      " [-0.9424666  -1.0902014 ]] [[-0.02427829]\n",
      " [-0.30185416]]\n",
      "Step 900 cost 0.6927982 [[-0.08790171 -0.13667817]\n",
      " [-0.9425567  -1.0924956 ]] [[-0.02500287]\n",
      " [-0.30604815]]\n",
      "Step 1000 cost 0.6926479 [[-0.09053397 -0.17406547]\n",
      " [-0.94265634 -1.096179  ]] [[-0.02559013]\n",
      " [-0.3142578 ]]\n",
      "Step 1100 cost 0.69247246 [[-0.09321532 -0.213312  ]\n",
      " [-0.9427644  -1.1013896 ]] [[-0.02596004]\n",
      " [-0.3266873 ]]\n",
      "Step 1200 cost 0.6922608 [[-0.09591798 -0.25499022]\n",
      " [-0.94287765 -1.1083398 ]] [[-0.0260255 ]\n",
      " [-0.34362411]]\n",
      "Step 1300 cost 0.69199914 [[-0.09860411 -0.29972807]\n",
      " [-0.9429909  -1.1173292 ]] [[-0.02568995]\n",
      " [-0.36544853]]\n",
      "Step 1400 cost 0.69166934 [[-0.10122503 -0.3482149 ]\n",
      " [-0.9431021  -1.1287534 ]] [[-0.02484474]\n",
      " [-0.3926444 ]]\n",
      "Step 1500 cost 0.6912482 [[-0.10371991 -0.40120515]\n",
      " [-0.94320524 -1.1431221 ]] [[-0.02336619]\n",
      " [-0.42580953]]\n",
      "Step 1600 cost 0.69070506 [[-0.10601437 -0.4595152 ]\n",
      " [-0.94329494 -1.1610731 ]] [[-0.02111231]\n",
      " [-0.4656653 ]]\n",
      "Step 1700 cost 0.69 [[-0.10801937 -0.52401274]\n",
      " [-0.9433662  -1.1833845 ]] [[-0.01791868]\n",
      " [-0.51306164]]\n",
      "Step 1800 cost 0.6890817 [[-0.10963003 -0.59559315]\n",
      " [-0.9434155  -1.2109826 ]] [[-0.01359358]\n",
      " [-0.5689749 ]]\n",
      "Step 1900 cost 0.68788517 [[-0.1107248 -0.6751417]\n",
      " [-0.9434417 -1.2449319]] [[-0.00791121]\n",
      " [-0.63449144]]\n",
      "Step 2000 cost 0.6863299 [[-0.1111654  -0.7634808 ]\n",
      " [-0.94344825 -1.286411  ]] [[-6.028145e-04]\n",
      " [-7.107742e-01]]\n",
      "Step 2100 cost 0.68431896 [[-0.11079706 -0.86130935]\n",
      " [-0.9434489  -1.3366578 ]] [[ 0.00865518]\n",
      " [-0.7990113 ]]\n",
      "Step 2200 cost 0.68173903 [[-0.10945006 -0.96914357]\n",
      " [-0.94347024 -1.396895  ]] [[ 0.02025646]\n",
      " [-0.9003441 ]]\n",
      "Step 2300 cost 0.6784618 [[-0.10694332 -1.0872798 ]\n",
      " [-0.9435628  -1.4682416 ]] [[ 0.03468248]\n",
      " [-1.0157825 ]]\n",
      "Step 2400 cost 0.67434573 [[-0.10309063 -1.2157888 ]\n",
      " [-0.9438124  -1.5516298 ]] [[ 0.05252007]\n",
      " [-1.1461172 ]]\n",
      "Step 2500 cost 0.6692399 [[-0.09771268 -1.3545417 ]\n",
      " [-0.9443595  -1.6477495 ]] [[ 0.07447361]\n",
      " [-1.2918278 ]]\n",
      "Step 2600 cost 0.66299367 [[-0.09065901 -1.5032426 ]\n",
      " [-0.9454271  -1.7570087 ]] [[ 0.10136349]\n",
      " [-1.4529816 ]]\n",
      "Step 2700 cost 0.6554759 [[-0.08184301 -1.6614331 ]\n",
      " [-0.94736016 -1.879505  ]] [[ 0.13409865]\n",
      " [-1.629104  ]]\n",
      "Step 2800 cost 0.6466105 [[-0.07129444 -1.8284246 ]\n",
      " [-0.9506717  -2.0149524 ]] [[ 0.17361163]\n",
      " [-1.8190262 ]]\n",
      "Step 2900 cost 0.63642234 [[-0.05922402 -2.0031636 ]\n",
      " [-0.9560931  -2.1625655 ]] [[ 0.22075208]\n",
      " [-2.0207362 ]]\n",
      "Step 3000 cost 0.6250777 [[-0.04608282 -2.1840665 ]\n",
      " [-0.9646075  -2.3209074 ]] [[ 0.27616093]\n",
      " [-2.231337  ]]\n",
      "Step 3100 cost 0.61288685 [[-0.03258365 -2.3689482 ]\n",
      " [-0.97743666 -2.4878178 ]] [[ 0.340177 ]\n",
      " [-2.4472098]]\n",
      "Step 3200 cost 0.6002544 [[-0.01964958 -2.555127  ]\n",
      " [-0.9959617  -2.6605194 ]] [[ 0.4128343]\n",
      " [-2.6644115]]\n",
      "Step 3300 cost 0.58759266 [[-0.0082797 -2.7397292]\n",
      " [-1.0215758 -2.835919 ]] [[ 0.4939646]\n",
      " [-2.879185 ]]\n",
      "Step 3400 cost 0.57523656 [[ 6.3325558e-04 -2.9200704e+00]\n",
      " [-1.0555007e+00 -3.0110278e+00]] [[ 0.58334917]\n",
      " [-3.0884256 ]]\n",
      "Step 3500 cost 0.5633958 [[ 0.00646541 -3.0939667 ]\n",
      " [-1.0986242  -3.183323  ]] [[ 0.68083584]\n",
      " [-3.2899454 ]]\n",
      "Step 3600 cost 0.552152 [[ 0.00893938 -3.2598803 ]\n",
      " [-1.1513965  -3.3509462 ]] [[ 0.78636926]\n",
      " [-3.4825122 ]]\n",
      "Step 3700 cost 0.5414846 [[ 0.0080845 -3.4169035]\n",
      " [-1.2137992 -3.512736 ]] [[ 0.8999344]\n",
      " [-3.6657286]]\n",
      "Step 3800 cost 0.53130907 [[ 0.00408921 -3.5646508 ]\n",
      " [-1.2853688  -3.668135  ]] [[ 1.0214566]\n",
      " [-3.8398345]]\n",
      "Step 3900 cost 0.5215115 [[-2.9132937e-03 -3.7031181e+00]\n",
      " [-1.3652371e+00 -3.8170497e+00]] [[ 1.1506989]\n",
      " [-4.0055017]]\n",
      "Step 4000 cost 0.5119767 [[-0.01306918 -3.8325555 ]\n",
      " [-1.4521558  -3.9597058 ]] [[ 1.2871869]\n",
      " [-4.1636624]]\n",
      "Step 4100 cost 0.50260603 [[-0.0269808 -3.9533746]\n",
      " [-1.5444806 -4.0965276]] [[ 1.4301734]\n",
      " [-4.315385 ]]\n",
      "Step 4200 cost 0.4933264 [[-0.045798  -4.0660877]\n",
      " [-1.6401181 -4.228033 ]] [[ 1.5786567]\n",
      " [-4.4617867]]\n",
      "Step 4300 cost 0.48408824 [[-0.07122289 -4.1712756 ]\n",
      " [-1.7364591  -4.3547616 ]] [[ 1.7314541]\n",
      " [-4.60398  ]]\n",
      "Step 4400 cost 0.47484946 [[-0.10547056 -4.2695622 ]\n",
      " [-1.8303263  -4.4772134 ]] [[ 1.8873416]\n",
      " [-4.7430487]]\n",
      "Step 4500 cost 0.46554238 [[-0.15126401 -4.3616247 ]\n",
      " [-1.9179447  -4.5958076 ]] [[ 2.0452619]\n",
      " [-4.8800416]]\n",
      "Step 4600 cost 0.45602143 [[-0.21194632 -4.448174  ]\n",
      " [-1.9949191  -4.7108445 ]] [[ 2.204598]\n",
      " [-5.015977]]\n",
      "Step 4700 cost 0.44597936 [[-0.2917581 -4.529972 ]\n",
      " [-2.0562212 -4.822464 ]] [[ 2.365529]\n",
      " [-5.15186 ]]\n",
      "Step 4800 cost 0.43481848 [[-0.39624375 -4.6078405 ]\n",
      " [-2.0963328  -4.930575  ]] [[ 2.5295155]\n",
      " [-5.288678 ]]\n",
      "Step 4900 cost 0.4214792 [[-0.5324813 -4.682699 ]\n",
      " [-2.1101735 -5.034734 ]] [[ 2.699903 ]\n",
      " [-5.4273515]]\n",
      "Step 5000 cost 0.40439367 [[-0.70803   -4.7556267]\n",
      " [-2.0964448 -5.133958 ]] [[ 2.882417]\n",
      " [-5.56848 ]]\n",
      "Step 5100 cost 0.38213274 [[-0.9262366 -4.8279085]\n",
      " [-2.0646403 -5.2267065]] [[ 3.0844777]\n",
      " [-5.711892 ]]\n",
      "Step 5200 cost 0.35496235 [[-1.1778983 -4.900805 ]\n",
      " [-2.0389507 -5.3114047]] [[ 3.3119278]\n",
      " [-5.8564425]]\n",
      "Step 5300 cost 0.32518774 [[-1.4400153 -4.9748936]\n",
      " [-2.0453641 -5.387445 ]] [[ 3.564543 ]\n",
      " [-6.0008597]]\n",
      "Step 5400 cost 0.29524514 [[-1.6893376 -5.049513 ]\n",
      " [-2.0938392 -5.4554467]] [[ 3.835127 ]\n",
      " [-6.1447086]]\n",
      "Step 5500 cost 0.26655918 [[-1.9141029 -5.123174 ]\n",
      " [-2.1783202 -5.516641 ]] [[ 4.1134067]\n",
      " [-6.2879515]]\n",
      "Step 5600 cost 0.23984906 [[-2.1129503 -5.1943684]\n",
      " [-2.2864797 -5.572273 ]] [[ 4.3903112]\n",
      " [-6.430193 ]]\n",
      "Step 5700 cost 0.2155104 [[-2.2890716 -5.2620583]\n",
      " [-2.4065585 -5.623344 ]] [[ 4.659522 ]\n",
      " [-6.5706244]]\n",
      "Step 5800 cost 0.19372395 [[-2.446344  -5.3257294]\n",
      " [-2.5298731 -5.6705985]] [[ 4.9172077]\n",
      " [-6.708316 ]]\n",
      "Step 5900 cost 0.17448497 [[-2.5879745 -5.3852654]\n",
      " [-2.650925  -5.714571 ]] [[ 5.161352 ]\n",
      " [-6.8424616]]\n",
      "Step 6000 cost 0.1576497 [[-2.716373  -5.4407864]\n",
      " [-2.7666643 -5.7556586]] [[ 5.3911657]\n",
      " [-6.972481 ]]\n",
      "Step 6100 cost 0.14299184 [[-2.833358  -5.4925427]\n",
      " [-2.8756669 -5.794174 ]] [[ 5.606682 ]\n",
      " [-7.0980144]]\n",
      "Step 6200 cost 0.13025327 [[-2.9403734 -5.5408316]\n",
      " [-2.9774804 -5.830372 ]] [[ 5.808422]\n",
      " [-7.218894]]\n",
      "Step 6300 cost 0.11917655 [[-3.0386043 -5.585962 ]\n",
      " [-3.0722017 -5.8644743]] [[ 5.9971914]\n",
      " [-7.3350897]]\n",
      "Step 6400 cost 0.10952345 [[-3.1290658 -5.6282277]\n",
      " [-3.1601987 -5.8966737]] [[ 6.173921 ]\n",
      " [-7.4466763]]\n",
      "Step 6500 cost 0.10108328 [[-3.2126298 -5.6679006]\n",
      " [-3.2419684 -5.9271398]] [[ 6.3395796]\n",
      " [-7.5537877]]\n",
      "Step 6600 cost 0.09367429 [[-3.2900593 -5.705231 ]\n",
      " [-3.3180413 -5.9560256]] [[ 6.4951153]\n",
      " [-7.656604 ]]\n",
      "Step 6700 cost 0.08714221 [[-3.362021  -5.74044  ]\n",
      " [-3.3889399 -5.9834657]] [[ 6.64142 ]\n",
      " [-7.755325]]\n",
      "Step 6800 cost 0.08135734 [[-3.4290962 -5.7737217]\n",
      " [-3.4551544 -6.009581 ]] [[ 6.7793193]\n",
      " [-7.8501635]]\n",
      "Step 6900 cost 0.0762112 [[-3.4917903 -5.80525  ]\n",
      " [-3.5171337 -6.0344787]] [[ 6.9095635]\n",
      " [-7.9413314]]\n",
      "Step 7000 cost 0.07161285 [[-3.5505474 -5.835181 ]\n",
      " [-3.5752823 -6.058257 ]] [[ 7.0328307]\n",
      " [-8.029037 ]]\n",
      "Step 7100 cost 0.06748639 [[-3.6057568 -5.8636518]\n",
      " [-3.6299632 -6.081001 ]] [[ 7.1497307]\n",
      " [-8.113487 ]]\n",
      "Step 7200 cost 0.063768014 [[-3.6577592 -5.8907824]\n",
      " [-3.681497  -6.10279  ]] [[ 7.2608104]\n",
      " [-8.194865 ]]\n",
      "Step 7300 cost 0.060404316 [[-3.7068558 -5.9166822]\n",
      " [-3.7301729 -6.123693 ]] [[ 7.366555]\n",
      " [-8.273354]]\n",
      "Step 7400 cost 0.057349913 [[-3.7533076 -5.941448 ]\n",
      " [-3.7762415 -6.1437755]] [[ 7.467402]\n",
      " [-8.349123]]\n",
      "Step 7500 cost 0.054566607 [[-3.7973506 -5.9651675]\n",
      " [-3.819932  -6.163093 ]] [[ 7.563745]\n",
      " [-8.422332]]\n",
      "Step 7600 cost 0.052021816 [[-3.8391874 -5.9879146]\n",
      " [-3.861443  -6.181695 ]] [[ 7.655931]\n",
      " [-8.493129]]\n",
      "Step 7700 cost 0.04968769 [[-3.8790028 -6.0097623]\n",
      " [-3.9009552 -6.199633 ]] [[ 7.744278]\n",
      " [-8.561649]]\n",
      "Step 7800 cost 0.047540504 [[-3.9169614 -6.0307713]\n",
      " [-3.9386277 -6.216945 ]] [[ 7.8290677]\n",
      " [-8.628022 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7900 cost 0.04555957 [[-3.9532082 -6.0510006]\n",
      " [-3.9746046 -6.2336736]] [[ 7.910554]\n",
      " [-8.692367]]\n",
      "Step 8000 cost 0.043727156 [[-3.9878733 -6.070501 ]\n",
      " [-4.009014  -6.2498527]] [[ 7.98897 ]\n",
      " [-8.754791]]\n",
      "Step 8100 cost 0.042027898 [[-4.0210752 -6.0893183]\n",
      " [-4.0419726 -6.2655163]] [[ 8.064522]\n",
      " [-8.815399]]\n",
      "Step 8200 cost 0.040448453 [[-4.052917  -6.1074963]\n",
      " [-4.073581  -6.2806926]] [[ 8.137401]\n",
      " [-8.874286]]\n",
      "Step 8300 cost 0.03897693 [[-4.083495  -6.125073 ]\n",
      " [-4.1039357 -6.2954097]] [[ 8.207779]\n",
      " [-8.931538]]\n",
      "Step 8400 cost 0.037603114 [[-4.1128964 -6.1420846]\n",
      " [-4.1331224 -6.3096933]] [[ 8.275812]\n",
      " [-8.987241]]\n",
      "Step 8500 cost 0.036317937 [[-4.1411977 -6.1585636]\n",
      " [-4.1612177 -6.323566 ]] [[ 8.341644]\n",
      " [-9.04147 ]]\n",
      "Step 8600 cost 0.035113327 [[-4.168471  -6.17454  ]\n",
      " [-4.1882925 -6.3370495]] [[ 8.405407]\n",
      " [-9.094297]]\n",
      "Step 8700 cost 0.03398224 [[-4.1947813 -6.190042 ]\n",
      " [-4.21441   -6.350164 ]] [[ 8.467218]\n",
      " [-9.145788]]\n",
      "Step 8800 cost 0.032918297 [[-4.220186  -6.2050953]\n",
      " [-4.2396307 -6.362928 ]] [[ 8.527191]\n",
      " [-9.196007]]\n",
      "Step 8900 cost 0.031915925 [[-4.244741  -6.2197227]\n",
      " [-4.2640066 -6.375359 ]] [[ 8.585426]\n",
      " [-9.245014]]\n",
      "Step 9000 cost 0.030970115 [[-4.268495  -6.233945 ]\n",
      " [-4.287589  -6.3874726]] [[ 8.642016]\n",
      " [-9.292858]]\n",
      "Step 9100 cost 0.030076224 [[-4.2914953 -6.2477846]\n",
      " [-4.3104205 -6.3992844]] [[ 8.697051]\n",
      " [-9.339596]]\n",
      "Step 9200 cost 0.029230352 [[-4.313783  -6.261259 ]\n",
      " [-4.3325443 -6.410807 ]] [[ 8.750609]\n",
      " [-9.385272]]\n",
      "Step 9300 cost 0.028428733 [[-4.3353972 -6.2743864]\n",
      " [-4.354001  -6.4220543]] [[ 8.802769]\n",
      " [-9.429933]]\n",
      "Step 9400 cost 0.027668163 [[-4.356374  -6.287184 ]\n",
      " [-4.374824  -6.4330397]] [[ 8.85359]\n",
      " [-9.47362]]\n",
      "Step 9500 cost 0.026945516 [[-4.376745  -6.299666 ]\n",
      " [-4.395046  -6.4437733]] [[ 8.903143]\n",
      " [-9.516377]]\n",
      "Step 9600 cost 0.026258275 [[-4.3965435 -6.3118463]\n",
      " [-4.414699  -6.454264 ]] [[ 8.951486]\n",
      " [-9.558239]]\n",
      "Step 9700 cost 0.025603827 [[-4.415796  -6.323738 ]\n",
      " [-4.4338107 -6.464524 ]] [[ 8.998676]\n",
      " [-9.59924 ]]\n",
      "Step 9800 cost 0.024979968 [[-4.4345303 -6.3353534]\n",
      " [-4.452408  -6.4745636]] [[ 9.044767]\n",
      " [-9.639414]]\n",
      "Step 9900 cost 0.0243847 [[-4.452771  -6.346706 ]\n",
      " [-4.4705143 -6.4843893]] [[ 9.089804]\n",
      " [-9.678794]]\n",
      "Step 10000 cost 0.023816053 [[-4.4705415 -6.357805 ]\n",
      " [-4.488154  -6.494012 ]] [[ 9.133835]\n",
      " [-9.71741 ]]\n",
      "hf\n",
      " [[0.01741559]\n",
      " [0.9789781 ]\n",
      " [0.97874445]\n",
      " [0.03436026]] \n",
      "pre\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "acc 1.0\n",
      "bias [6.6196184 2.5532248] [-4.1387725]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: xdata, y: ydata})\n",
    "        if step%100 == 0:\n",
    "            print(\"Step\", step, \"cost\", sess.run(cost, feed_dict={x : xdata, y: ydata}),\n",
    "                 sess.run(w1), sess.run(w2))\n",
    "    hv, pv, av = sess.run([hf, predict, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"hf\\n\", hv, \"\\npre\\n\", pv, \"\\nacc\", av)\n",
    "    print(\"bias\", sess.run(b1), sess.run(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 10]))\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "L1 = tf.sigmoid(tf.matmul(x, w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10, 1]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "hf = tf.sigmoid(tf.matmul(L1, w2)+b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "predict = tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 cost 0.79134715\n",
      "Step 100 cost 0.6898974\n",
      "Step 200 cost 0.6747861\n",
      "Step 300 cost 0.6596863\n",
      "Step 400 cost 0.6427167\n",
      "Step 500 cost 0.62273157\n",
      "Step 600 cost 0.5990296\n",
      "Step 700 cost 0.5711899\n",
      "Step 800 cost 0.5390243\n",
      "Step 900 cost 0.5026497\n",
      "Step 1000 cost 0.46261314\n",
      "Step 1100 cost 0.41996863\n",
      "Step 1200 cost 0.37621439\n",
      "Step 1300 cost 0.333064\n",
      "Step 1400 cost 0.2921316\n",
      "Step 1500 cost 0.25465426\n",
      "Step 1600 cost 0.22134987\n",
      "Step 1700 cost 0.19243032\n",
      "Step 1800 cost 0.1677221\n",
      "Step 1900 cost 0.14682102\n",
      "Step 2000 cost 0.12922536\n",
      "Step 2100 cost 0.11442533\n",
      "Step 2200 cost 0.10195317\n",
      "Step 2300 cost 0.09140291\n",
      "Step 2400 cost 0.08243406\n",
      "Step 2500 cost 0.07476668\n",
      "Step 2600 cost 0.06817297\n",
      "Step 2700 cost 0.062468383\n",
      "Step 2800 cost 0.057503693\n",
      "Step 2900 cost 0.05315817\n",
      "Step 3000 cost 0.049333405\n",
      "Step 3100 cost 0.045949325\n",
      "Step 3200 cost 0.042940244\n",
      "Step 3300 cost 0.04025203\n",
      "Step 3400 cost 0.037839875\n",
      "Step 3500 cost 0.035666298\n",
      "Step 3600 cost 0.033700116\n",
      "Step 3700 cost 0.03191502\n",
      "Step 3800 cost 0.030288655\n",
      "Step 3900 cost 0.02880212\n",
      "Step 4000 cost 0.027439177\n",
      "Step 4100 cost 0.026186004\n",
      "Step 4200 cost 0.02503058\n",
      "Step 4300 cost 0.023962567\n",
      "Step 4400 cost 0.02297293\n",
      "Step 4500 cost 0.022053808\n",
      "Step 4600 cost 0.021198343\n",
      "Step 4700 cost 0.02040049\n",
      "Step 4800 cost 0.019654915\n",
      "Step 4900 cost 0.018956864\n",
      "Step 5000 cost 0.01830222\n",
      "Step 5100 cost 0.017687157\n",
      "Step 5200 cost 0.017108385\n",
      "Step 5300 cost 0.016563006\n",
      "Step 5400 cost 0.01604825\n",
      "Step 5500 cost 0.015561828\n",
      "Step 5600 cost 0.015101487\n",
      "Step 5700 cost 0.014665285\n",
      "Step 5800 cost 0.014251476\n",
      "Step 5900 cost 0.013858419\n",
      "Step 6000 cost 0.013484727\n",
      "Step 6100 cost 0.013128987\n",
      "Step 6200 cost 0.012790033\n",
      "Step 6300 cost 0.012466715\n",
      "Step 6400 cost 0.012158051\n",
      "Step 6500 cost 0.011863123\n",
      "Step 6600 cost 0.011581027\n",
      "Step 6700 cost 0.011310984\n",
      "Step 6800 cost 0.011052261\n",
      "Step 6900 cost 0.01080426\n",
      "Step 7000 cost 0.0105662355\n",
      "Step 7100 cost 0.010337743\n",
      "Step 7200 cost 0.010118144\n",
      "Step 7300 cost 0.009906994\n",
      "Step 7400 cost 0.009703822\n",
      "Step 7500 cost 0.009508189\n",
      "Step 7600 cost 0.009319726\n",
      "Step 7700 cost 0.009138038\n",
      "Step 7800 cost 0.008962806\n",
      "Step 7900 cost 0.008793667\n",
      "Step 8000 cost 0.008630375\n",
      "Step 8100 cost 0.008472598\n",
      "Step 8200 cost 0.008319987\n",
      "Step 8300 cost 0.008172509\n",
      "Step 8400 cost 0.008029741\n",
      "Step 8500 cost 0.007891548\n",
      "Step 8600 cost 0.0077576833\n",
      "Step 8700 cost 0.007627969\n",
      "Step 8800 cost 0.007502281\n",
      "Step 8900 cost 0.007380319\n",
      "Step 9000 cost 0.0072619896\n",
      "Step 9100 cost 0.0071471576\n",
      "Step 9200 cost 0.007035657\n",
      "Step 9300 cost 0.006927395\n",
      "Step 9400 cost 0.0068221325\n",
      "Step 9500 cost 0.006719822\n",
      "Step 9600 cost 0.0066202977\n",
      "Step 9700 cost 0.0065235593\n",
      "Step 9800 cost 0.0064293807\n",
      "Step 9900 cost 0.006337701\n",
      "Step 10000 cost 0.0062484597\n",
      "hf\n",
      " [[0.00463241]\n",
      " [0.9944079 ]\n",
      " [0.99297804]\n",
      " [0.00766657]] \n",
      "pre\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "acc 1.0\n",
      "bias [ 2.1334443   0.29051885 -0.96358424 -1.2852647  -0.99203724  1.914707\n",
      " -1.5763075   5.163971    0.7600645  -0.34769943] [0.4783285]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: xdata, y: ydata})\n",
    "        if step%100 == 0:\n",
    "            print(\"Step\", step, \"cost\", sess.run(cost, feed_dict={x : xdata, y: ydata}))\n",
    "    hv, pv, av = sess.run([hf, predict, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "#     print(\"hf\\n\", hv, \"\\npre\\n\", pv, \"\\nacc\", av)\n",
    "    print(\"hf\\n\", hv, \"\\npre\\n\", pv, \"\\nacc\", av)\n",
    "    print(\"bias\", sess.run(b1), sess.run(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 10]))\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "L1 = tf.sigmoid(tf.matmul(x, w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b2 = tf.Variable(tf.random_normal([10]))\n",
    "L2 = tf.sigmoid(tf.matmul(L1, w2)+b2)\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "L3 = tf.sigmoid(tf.matmul(L2, w3)+b3)\n",
    "\n",
    "w4 = tf.Variable(tf.random_normal([10, 1]))\n",
    "b4 = tf.Variable(tf.random_normal([1]))\n",
    "hf = tf.sigmoid(tf.matmul(L3, w4)+b4)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "predict = tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 cost 1.0466444\n",
      "Step 100 cost 0.6947388\n",
      "Step 200 cost 0.69427556\n",
      "Step 300 cost 0.69385993\n",
      "Step 400 cost 0.6934739\n",
      "Step 500 cost 0.6931021\n",
      "Step 600 cost 0.6927302\n",
      "Step 700 cost 0.6923446\n",
      "Step 800 cost 0.69193095\n",
      "Step 900 cost 0.69147336\n",
      "Step 1000 cost 0.6909522\n",
      "Step 1100 cost 0.69034314\n",
      "Step 1200 cost 0.68961394\n",
      "Step 1300 cost 0.68872064\n",
      "Step 1400 cost 0.6876015\n",
      "Step 1500 cost 0.68616796\n",
      "Step 1600 cost 0.6842892\n",
      "Step 1700 cost 0.68176734\n",
      "Step 1800 cost 0.67829144\n",
      "Step 1900 cost 0.67335856\n",
      "Step 2000 cost 0.6661247\n",
      "Step 2100 cost 0.6551393\n",
      "Step 2200 cost 0.63788456\n",
      "Step 2300 cost 0.6101165\n",
      "Step 2400 cost 0.56552166\n",
      "Step 2500 cost 0.49781865\n",
      "Step 2600 cost 0.4083906\n",
      "Step 2700 cost 0.3130074\n",
      "Step 2800 cost 0.230959\n",
      "Step 2900 cost 0.16993967\n",
      "Step 3000 cost 0.1274937\n",
      "Step 3100 cost 0.098361924\n",
      "Step 3200 cost 0.07808703\n",
      "Step 3300 cost 0.06362111\n",
      "Step 3400 cost 0.053013407\n",
      "Step 3500 cost 0.045027565\n",
      "Step 3600 cost 0.03887002\n",
      "Step 3700 cost 0.03401978\n",
      "Step 3800 cost 0.03012671\n",
      "Step 3900 cost 0.02694961\n",
      "Step 4000 cost 0.024318328\n",
      "Step 4100 cost 0.022110747\n",
      "Step 4200 cost 0.020237079\n",
      "Step 4300 cost 0.018630505\n",
      "Step 4400 cost 0.01724011\n",
      "Step 4500 cost 0.016026884\n",
      "Step 4600 cost 0.0149603505\n",
      "Step 4700 cost 0.01401639\n",
      "Step 4800 cost 0.013175855\n",
      "Step 4900 cost 0.012423233\n",
      "Step 5000 cost 0.01174584\n",
      "Step 5100 cost 0.011133413\n",
      "Step 5200 cost 0.010577247\n",
      "Step 5300 cost 0.010070218\n",
      "Step 5400 cost 0.009606295\n",
      "Step 5500 cost 0.009180396\n",
      "Step 5600 cost 0.00878811\n",
      "Step 5700 cost 0.008425808\n",
      "Step 5800 cost 0.008090196\n",
      "Step 5900 cost 0.007778575\n",
      "Step 6000 cost 0.0074885353\n",
      "Step 6100 cost 0.0072179264\n",
      "Step 6200 cost 0.0069649173\n",
      "Step 6300 cost 0.0067279493\n",
      "Step 6400 cost 0.0065055103\n",
      "Step 6500 cost 0.0062964074\n",
      "Step 6600 cost 0.006099477\n",
      "Step 6700 cost 0.005913737\n",
      "Step 6800 cost 0.0057382686\n",
      "Step 6900 cost 0.005572226\n",
      "Step 7000 cost 0.005414945\n",
      "Step 7100 cost 0.0052657784\n",
      "Step 7200 cost 0.0051240483\n",
      "Step 7300 cost 0.0049893316\n",
      "Step 7400 cost 0.0048610717\n",
      "Step 7500 cost 0.004738802\n",
      "Step 7600 cost 0.004622192\n",
      "Step 7700 cost 0.0045107882\n",
      "Step 7800 cost 0.004404367\n",
      "Step 7900 cost 0.0043025063\n",
      "Step 8000 cost 0.0042049955\n",
      "Step 8100 cost 0.0041115345\n",
      "Step 8200 cost 0.004021882\n",
      "Step 8300 cost 0.003935798\n",
      "Step 8400 cost 0.0038531325\n",
      "Step 8500 cost 0.0037736143\n",
      "Step 8600 cost 0.003697154\n",
      "Step 8700 cost 0.0036235857\n",
      "Step 8800 cost 0.00355267\n",
      "Step 8900 cost 0.003484331\n",
      "Step 9000 cost 0.003418389\n",
      "Step 9100 cost 0.0033547692\n",
      "Step 9200 cost 0.0032933508\n",
      "Step 9300 cost 0.0032339545\n",
      "Step 9400 cost 0.0031765797\n",
      "Step 9500 cost 0.0031210766\n",
      "Step 9600 cost 0.0030673705\n",
      "Step 9700 cost 0.003015386\n",
      "Step 9800 cost 0.0029650035\n",
      "Step 9900 cost 0.0029161775\n",
      "Step 10000 cost 0.0028688782\n",
      "hf\n",
      " [[0.00289595]\n",
      " [0.99689436]\n",
      " [0.9974347 ]\n",
      " [0.00289209]] \n",
      "pre\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "acc 1.0\n",
      "bias [ 0.12054174 -0.9255871  -0.00136528  0.6608738   0.76467407  1.2406616\n",
      " -0.5165288   1.0984516   0.38557354 -0.41977322] [ 0.4743905   0.20481405 -0.931253    0.4932572  -0.3250825   1.5044581\n",
      "  0.75326544  0.98811203 -0.8209026  -2.4240422 ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: xdata, y: ydata})\n",
    "        if step%100 == 0:\n",
    "            print(\"Step\", step, \"cost\", sess.run(cost, feed_dict={x : xdata, y: ydata}))\n",
    "    hv, pv, av = sess.run([hf, predict, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "#     print(\"hf\\n\", hv, \"\\npre\\n\", pv, \"\\nacc\", av)\n",
    "    print(\"hf\\n\", hv, \"\\npre\\n\", pv, \"\\nacc\", av)\n",
    "    print(\"bias\", sess.run(b1), sess.run(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 10]))\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "L1 = tf.sigmoid(tf.matmul(x, w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b2 = tf.Variable(tf.random_normal([10]))\n",
    "L2 = tf.sigmoid(tf.matmul(L1, w2)+b2)\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "L3 = tf.sigmoid(tf.matmul(L2, w3)+b3)\n",
    "\n",
    "w4 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "L4 = tf.sigmoid(tf.matmul(L3, w4)+b4)\n",
    "\n",
    "w5 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "L5 = tf.sigmoid(tf.matmul(L4, w5)+b5)\n",
    "\n",
    "w6 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b6 = tf.Variable(tf.random_normal([10]))\n",
    "L6 = tf.sigmoid(tf.matmul(L5, w6)+b6)\n",
    "\n",
    "w7 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b7 = tf.Variable(tf.random_normal([10]))\n",
    "L7 = tf.sigmoid(tf.matmul(L6, w7)+b7)\n",
    "\n",
    "w8 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "L8 = tf.sigmoid(tf.matmul(L7, w8)+b8)\n",
    "\n",
    "w9 = tf.Variable(tf.random_normal([10, 10]))\n",
    "b9 = tf.Variable(tf.random_normal([10]))\n",
    "L9 = tf.sigmoid(tf.matmul(L8, w9)+b9)\n",
    "\n",
    "w10 = tf.Variable(tf.random_normal([10, 1]))\n",
    "b10 = tf.Variable(tf.random_normal([1]))\n",
    "hf = tf.sigmoid(tf.matmul(L9, w10)+b10)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hf)+(1-y)*tf.log(1-hf))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\n",
    "predict = tf.cast(hf>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 cost 0.84387153\n",
      "Step 100 cost 0.69313806\n",
      "Step 200 cost 0.6931379\n",
      "Step 300 cost 0.69313776\n",
      "Step 400 cost 0.6931376\n",
      "Step 500 cost 0.6931374\n",
      "Step 600 cost 0.69313717\n",
      "Step 700 cost 0.69313705\n",
      "Step 800 cost 0.69313693\n",
      "Step 900 cost 0.6931367\n",
      "Step 1000 cost 0.69313645\n",
      "Step 1100 cost 0.6931364\n",
      "Step 1200 cost 0.6931362\n",
      "Step 1300 cost 0.6931359\n",
      "Step 1400 cost 0.69313586\n",
      "Step 1500 cost 0.6931357\n",
      "Step 1600 cost 0.6931355\n",
      "Step 1700 cost 0.6931353\n",
      "Step 1800 cost 0.6931351\n",
      "Step 1900 cost 0.693135\n",
      "Step 2000 cost 0.69313484\n",
      "Step 2100 cost 0.6931346\n",
      "Step 2200 cost 0.6931344\n",
      "Step 2300 cost 0.6931341\n",
      "Step 2400 cost 0.693134\n",
      "Step 2500 cost 0.6931339\n",
      "Step 2600 cost 0.6931336\n",
      "Step 2700 cost 0.6931335\n",
      "Step 2800 cost 0.69313335\n",
      "Step 2900 cost 0.6931331\n",
      "Step 3000 cost 0.693133\n",
      "Step 3100 cost 0.69313276\n",
      "Step 3200 cost 0.6931326\n",
      "Step 3300 cost 0.69313234\n",
      "Step 3400 cost 0.69313216\n",
      "Step 3500 cost 0.69313204\n",
      "Step 3600 cost 0.6931318\n",
      "Step 3700 cost 0.6931316\n",
      "Step 3800 cost 0.69313145\n",
      "Step 3900 cost 0.69313127\n",
      "Step 4000 cost 0.6931311\n",
      "Step 4100 cost 0.69313097\n",
      "Step 4200 cost 0.6931307\n",
      "Step 4300 cost 0.69313055\n",
      "Step 4400 cost 0.6931304\n",
      "Step 4500 cost 0.6931302\n",
      "Step 4600 cost 0.69312996\n",
      "Step 4700 cost 0.6931297\n",
      "Step 4800 cost 0.69312966\n",
      "Step 4900 cost 0.6931294\n",
      "Step 5000 cost 0.69312924\n",
      "Step 5100 cost 0.693129\n",
      "Step 5200 cost 0.69312876\n",
      "Step 5300 cost 0.6931286\n",
      "Step 5400 cost 0.6931284\n",
      "Step 5500 cost 0.6931281\n",
      "Step 5600 cost 0.693128\n",
      "Step 5700 cost 0.6931278\n",
      "Step 5800 cost 0.69312763\n",
      "Step 5900 cost 0.69312733\n",
      "Step 6000 cost 0.69312716\n",
      "Step 6100 cost 0.693127\n",
      "Step 6200 cost 0.6931268\n",
      "Step 6300 cost 0.69312656\n",
      "Step 6400 cost 0.6931263\n",
      "Step 6500 cost 0.69312614\n",
      "Step 6600 cost 0.693126\n",
      "Step 6700 cost 0.69312567\n",
      "Step 6800 cost 0.6931254\n",
      "Step 6900 cost 0.6931253\n",
      "Step 7000 cost 0.6931251\n",
      "Step 7100 cost 0.6931249\n",
      "Step 7200 cost 0.6931247\n",
      "Step 7300 cost 0.69312435\n",
      "Step 7400 cost 0.6931241\n",
      "Step 7500 cost 0.69312394\n",
      "Step 7600 cost 0.6931238\n",
      "Step 7700 cost 0.6931235\n",
      "Step 7800 cost 0.69312334\n",
      "Step 7900 cost 0.69312304\n",
      "Step 8000 cost 0.6931228\n",
      "Step 8100 cost 0.69312257\n",
      "Step 8200 cost 0.6931223\n",
      "Step 8300 cost 0.69312215\n",
      "Step 8400 cost 0.69312185\n",
      "Step 8500 cost 0.6931217\n",
      "Step 8600 cost 0.69312143\n",
      "Step 8700 cost 0.6931212\n",
      "Step 8800 cost 0.69312096\n",
      "Step 8900 cost 0.6931206\n",
      "Step 9000 cost 0.6931204\n",
      "Step 9100 cost 0.6931202\n",
      "Step 9200 cost 0.6931198\n",
      "Step 9300 cost 0.6931197\n",
      "Step 9400 cost 0.69311947\n",
      "Step 9500 cost 0.6931191\n",
      "Step 9600 cost 0.6931189\n",
      "Step 9700 cost 0.69311863\n",
      "Step 9800 cost 0.6931184\n",
      "Step 9900 cost 0.6931181\n",
      "Step 10000 cost 0.6931178\n",
      "hf\n",
      " [[0.50007284]\n",
      " [0.49994135]\n",
      " [0.50010043]\n",
      " [0.49991027]] \n",
      "pre\n",
      " [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]] \n",
      "acc 0.5\n",
      "bias [ 0.56796813 -0.04766957 -0.29912657  0.60116595 -0.92112374 -0.6824805\n",
      " -0.26377672  0.6842567  -0.11300658 -0.99027246] [ 1.4239606   0.9689087   0.4506207  -0.07963947  0.18318781  0.61755234\n",
      " -0.90662366 -0.3495996  -1.803743   -0.09853552]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: xdata, y: ydata})\n",
    "        if step%100 == 0:\n",
    "            print(\"Step\", step, \"cost\", sess.run(cost, feed_dict={x : xdata, y: ydata}))\n",
    "    hv, pv, av = sess.run([hf, predict, accuracy], feed_dict={x: xdata, y: ydata})\n",
    "    print(\"hf\\n\", hv, \"\\npre\\n\", pv, \"\\nacc\", av)\n",
    "    print(\"bias\", sess.run(b1), sess.run(b2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
